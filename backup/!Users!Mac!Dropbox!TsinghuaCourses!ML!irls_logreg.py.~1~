from sklearn.datasets import load_svmlight_file
import numpy as np
import os
import math

path_train     = os.getcwd() + '/a9a/a9a'
path_test      = os.getcwd() + '/a9a/a9a.t'

# load all data
# X type: scipy.sparse.csr.csr_matrix
X_train, y_train = load_svmlight_file(path_train, n_features=123, dtype=np.float32)
X_test, y_test = load_svmlight_file(path_test, n_features=123, dtype=np.float32)

# X_train: (32561, 123), y_train: (32561,)
# X_test:  (16281, 123), y_test:(16281,)

# make sure that we have numpy arrays; also
# reshape the array X to ensure that we have
# a multidimensional numpy array (ndarray)
#X_train = np.array(X_train).reshape((X_train.shape[0], -1))
#t = np.array(t).reshape((len(t),1))

#Prepend a column of ones
#train_ones = np.ones((X_train.shape[0], 1))
#X_train = np.concatenate((train_ones, X_train), axis=1)

#test_ones = np.ones((X_test.shape[0], 1))
#X_test = np.concatenate((test_ones, X_test), axis=1)
# stack a dimension of ones to X to simplify computation
N_train = X_train.shape[0]
N_test = X_test.shape[0]
X_train = np.hstack((np.ones((N_train, 1)), X_train.toarray()))
X_test = np.hstack((np.ones((N_test, 1)), X_test.toarray()))

def S(x):
    return 1 / (1 + math.exp(-x))

def u(x):
    return S(x)

#Rii
def R_mat(X,w):
    rows = X.shape[0]
    diag = np.zeros(rows)
    boldmu = np.zeros(rows)

    for i in range(rows):
        print("R_mat", i)
        xi = X[i]
        wTxi = np.dot(w.T, xi)
        u = S(wTxi)
        boldmu[i] = u
        diag[i] = u * (1 - u)
    print("hej")
    R = np.diagflat(diag)

    return R, boldmu


def gradL(w, boldmu, y):
    return np.dot(X, (y - boldmu))

#y =  prediction
#BOLDMU???
def main(X, y):
    MAX_ITER = 1000
    THR = 0.000001



    M, N = X.shape

    R = np.zeros((N,N))
    #W init, wt
    w = np.zeros(N)

    for i in range(MAX_ITER):
        R, boldmu = R_mat(X,w)

        #(XRXt)
        d = np.dot(np.dot(X.T, R), X)

        #(XRXt)^(-1)
        a = np.linalg.inv(d)

        #XR
        b = np.dot(X.T, R)

        #(XRXt)^(-1)XR
        c = np.dot(a, b)
        z = np.dot(X, w) - np.dot(np.linalg.inv(R), (y - boldmu))

        w = c*z
        print("steps", i)
        if gradL(w, boldmu, y) < thr:
            break

    return w


model = main(X_train, y_train)
